{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IBM_Instrument_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6HO3ds-e1D1"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-IkMGEOMsup"
      },
      "source": [
        "from librosa.core import load as ld_wav\n",
        "from librosa.feature import delta\n",
        "import librosa.feature as ft_extraction\n",
        "import scipy.io.wavfile as wav\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix,make_scorer,f1_score\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold,cross_validate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.kernel_approximation import Nystroem\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn import metrics\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "!pip install scikit-optimize\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from operator import itemgetter\n",
        "import pickle\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.metrics import AUC,Precision,Recall,BinaryCrossentropy,Accuracy\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMH0bqLMIHzA"
      },
      "source": [
        "## Install pyspark and systemml"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKS4nhs_C6nF"
      },
      "source": [
        "# instalar as dependências\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        " \n",
        "# tornar o pyspark \"importável\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')\n",
        "!pip install elephas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiGIOsUxGQ2-"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SQLContext, SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfnfNoygnGVQ"
      },
      "source": [
        "# Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVcIrKZ7P4J3"
      },
      "source": [
        "!ls \"/content/drive/My Drive/IRMAS-training\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV4vqbs7Msu4"
      },
      "source": [
        "# directory where we your .wav files are\n",
        "directoryName = \"/content/drive/My Drive/IRMAS-training\" # put your own directory here\n",
        "#instruments to evaluate\n",
        "instruments = [\"pia\",\"vio\"]\n",
        "# directory to put our results in, you can change the name if you like\n",
        "resultsDirectory = directoryName + \"/MFCCresults\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUo6_H6BMsvG"
      },
      "source": [
        "def countTrainTracks(input_path, labels):\n",
        "\t\t\"\"\" Counts the number of tracks in the folders of the trainset\n",
        "\t\t\"\"\"\n",
        "\t\ttotal = 0\n",
        "\t\tfor l, label in enumerate(labels):\n",
        "\t\t\tinstrument_dir = os.path.join(input_path, label)\n",
        "\t\t\ttotal += len(os.listdir(instrument_dir))\n",
        "\n",
        "\t\treturn total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o86Q89gCMsvP"
      },
      "source": [
        "if not os.path.exists(resultsDirectory):\n",
        "    os.makedirs(resultsDirectory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TCCt4FeMsvV"
      },
      "source": [
        "total_tracks = countTrainTracks(directoryName,instruments)\n",
        "print(\"Total tracks: \",total_tracks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRuSAeJPMsvf"
      },
      "source": [
        "data_dict = dict()\n",
        "data_dict[\"rolloff\"] = list()\n",
        "data_dict[\"bandwidth\"] = list()\n",
        "data_dict[\"centroids\"] = list()\n",
        "data_dict[\"zero_crossing_rate\"] = list()\n",
        "data_dict[\"rms\"] = list()\n",
        "data_dict[\"slope\"] = list()\n",
        "data_dict[\"kurtosis\"] = list()\n",
        "data_dict[\"skewness\"] = list()\n",
        "for i in range(0,13):\n",
        "    data_dict[\"mfcc\"+str(i)] = list()\n",
        "data_dict[\"instrument\"] = list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KERczuLpMsvo"
      },
      "source": [
        "#Essa funcao recebe cada coluna mfcc[j], que é um vetor onde cada elemento vetor[i]\n",
        "#contem todos os K coeficientes mfcc[j] de um audio i. O objetivo é descobrir como os MFCC[j] mudam ao longo do tempo.\n",
        "#Obs: \n",
        "#j (quantidade de coeficientes MFCC) = {0,13}, \n",
        "#k (quantidade de uma mesma caracteristica extraída do audio completo - depende do tamanho de janela e do tempo de duração) = {0,130}, \n",
        "#i (numero de arquivos do dataset) = 1301 \n",
        "def getDeltaFeat(column):\n",
        "    #Quantos vetores serão lidos, ou seja, o tamanho do dataset \n",
        "    original_len = len(column)\n",
        "    deltas = list()\n",
        "    for i in range(original_len):\n",
        "      deltas.append(delta(column[i], order=1))\n",
        "    return np.array(deltas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQW0c7QWMsvu"
      },
      "source": [
        "def includeDeltaFeat(df):\n",
        "    for i in range(0,13):\n",
        "        df[\"delta_mfcc\"+str(i)]=getDeltaFeat(df[\"mfcc\"+str(i)])\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XID-lO5QdSrN"
      },
      "source": [
        "def FeatureSpectralCentroid(X, f_s):\n",
        "\n",
        "    isSpectrum = X.ndim == 1\n",
        "\n",
        "    # X = X**2 removed for consistency with book\n",
        "\n",
        "    norm = X.sum(axis=0, keepdims=True)\n",
        "    norm[norm == 0] = 1\n",
        "\n",
        "    vsc = np.dot(np.arange(0, X.shape[0]), X) / norm\n",
        "\n",
        "    # convert from index to Hz\n",
        "    vsc = vsc / (X.shape[0] - 1) * f_s / 2\n",
        "\n",
        "    # if input is a spectrum, output scaler else if spectrogram, output 1d array\n",
        "    vsc = np.squeeze(vsc) if isSpectrum else np.squeeze(vsc, axis=0)\n",
        "\n",
        "    return vsc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq-KqdFqdbWb"
      },
      "source": [
        "def FeatureSpectralSpread(X, f_s):\n",
        "\n",
        "    isSpectrum = X.ndim == 1\n",
        "    if isSpectrum:\n",
        "        X = np.expand_dims(X, axis=1)\n",
        "\n",
        "    # get spectral centroid as index\n",
        "    vsc = FeatureSpectralCentroid(X, f_s) * 2 / f_s * (X.shape[0] - 1)\n",
        "\n",
        "    # X = X**2 removed for consistency with book\n",
        "\n",
        "    norm = X.sum(axis=0)\n",
        "    norm[norm == 0] = 1\n",
        "\n",
        "    # compute spread\n",
        "    vss = np.zeros(X.shape[1])\n",
        "    indices = np.arange(0, X.shape[0])\n",
        "    for n in range(0, X.shape[1]):\n",
        "        vss[n] = np.dot((indices - vsc[n])**2, X[:, n]) / norm[n]\n",
        "\n",
        "    vss = np.sqrt(vss)\n",
        "\n",
        "    # convert from index to Hz\n",
        "    vss = vss / (X.shape[0] - 1) * f_s / 2\n",
        "\n",
        "    return np.squeeze(vss) if isSpectrum else vss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pxci8ihKc7yQ"
      },
      "source": [
        "def FeatureSpectralKurtosis(X, f_s, UseBookDefinition=False):\n",
        "\n",
        "    isSpectrum = X.ndim == 1\n",
        "    if isSpectrum:\n",
        "        X = np.expand_dims(X, axis=1)\n",
        "\n",
        "    if UseBookDefinition:  # not recommended\n",
        "        # compute mean and standard deviation\n",
        "        mu_x = np.mean(X, axis=0, keepdims=True)\n",
        "        std_x = np.std(X, axis=0)\n",
        "\n",
        "        # remove mean\n",
        "        X = X - mu_x\n",
        "\n",
        "        # compute kurtosis\n",
        "        vsk = np.sum(X**4, axis=0) / (std_x**4 * X.shape[0])\n",
        "    else:\n",
        "        f = np.arange(0, X.shape[0]) / (X.shape[0] - 1) * f_s / 2\n",
        "        # get spectral centroid and spread (mean and std of dist)\n",
        "        vsc = FeatureSpectralCentroid(X, f_s)  # *2/f_s * (X.shape[0]-1)\n",
        "        vss = FeatureSpectralSpread(X, f_s)    # *2/f_s * (X.shape[0]-1)\n",
        "\n",
        "        norm = X.sum(axis=0)\n",
        "        norm[norm == 0] = 1\n",
        "        vss[vss == 0] = 1\n",
        "\n",
        "        # compute kurtosis\n",
        "        vsk = np.zeros(X.shape[1])\n",
        "        for n in range(0, X.shape[1]):\n",
        "            vsk[n] = np.dot((f - vsc[n])**4, X[:, n]) / (vss[n]**4 * norm[n] * X.shape[0])\n",
        "\n",
        "    return np.squeeze(vsk - 3) if isSpectrum else (vsk - 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j94lRZqzrRrj"
      },
      "source": [
        "def FeatureSpectralSkewness(X, f_s, UseBookDefinition=False):\n",
        "\n",
        "    isSpectrum = X.ndim == 1\n",
        "    if isSpectrum:\n",
        "        X = np.expand_dims(X, axis=1)\n",
        "\n",
        "    if UseBookDefinition:  # not recommended\n",
        "        # compute mean and standard deviation\n",
        "        mu_x = np.mean(X, axis=0, keepdims=True)\n",
        "        std_x = np.std(X, axis=0)\n",
        "\n",
        "        # remove mean\n",
        "        X = X - mu_x\n",
        "\n",
        "        # compute kurtosis\n",
        "        vssk = np.sum(X**3, axis=0) / (std_x**3 * X.shape[0])\n",
        "    else:\n",
        "        f = np.arange(0, X.shape[0]) / (X.shape[0] - 1) * f_s / 2\n",
        "        # get spectral centroid and spread (mean and std of dist)\n",
        "        vsc = FeatureSpectralCentroid(X, f_s) \n",
        "        vss = FeatureSpectralSpread(X, f_s)   \n",
        "\n",
        "        norm = X.sum(axis=0)\n",
        "        norm[norm == 0] = 1\n",
        "        vss[vss == 0] = 1\n",
        "\n",
        "        # compute spread\n",
        "        vssk = np.zeros(X.shape[1])\n",
        "        for n in range(0, X.shape[1]):\n",
        "            vssk[n] = np.dot((f - vsc[n])**3, X[:, n]) / (vss[n]**3 * norm[n] * X.shape[0])\n",
        "\n",
        "    return np.squeeze(vssk) if isSpectrum else vssk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94GrfCkSfm_P"
      },
      "source": [
        "def FeatureSpectralSlope(X, f_s):\n",
        "\n",
        "    # compute mean\n",
        "    mu_x = X.mean(axis=0, keepdims=True)\n",
        "\n",
        "    # compute index vector\n",
        "    kmu = np.arange(0, X.shape[0]) - X.shape[0] / 2\n",
        "\n",
        "    # compute slope\n",
        "    X = X - mu_x\n",
        "    vssl = np.dot(kmu, X) / np.dot(kmu, kmu)\n",
        "\n",
        "    return vssl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrZz8n0RMsxc"
      },
      "source": [
        "(sig,rate) = ld_wav(directoryName +\"/\"+'pia'+\"/\" +'001__[pia][nod][cla]1389__1.wav')\n",
        "spectogram = np.abs(librosa.stft(sig,n_fft=512))\n",
        "feat = FeatureSpectralSlope(spectogram,rate)\n",
        "print(np.array(feat).reshape(1,-1).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq7oICcjKNLz"
      },
      "source": [
        "mfcc_feat = ft_extraction.mfcc(S=spectogram,sr=rate,n_mfcc=13)\n",
        "mfcc_feat.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF5TMDqiy82s"
      },
      "source": [
        "ft_extraction.zero_crossing_rate(y=sig,frame_length=512,hop_length=512//4).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRYhxwOAOe2-"
      },
      "source": [
        "len(sig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpnILXHeOJV8"
      },
      "source": [
        "def zeroPadding(sig,wav_len = 66150):\n",
        "  left_zeros = np.zeros(wav_len - sig.shape[0])\n",
        "  return np.hstack([left_zeros,sig])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3hExKkqMsv0"
      },
      "source": [
        "def getData(directoryName,instruments,data_dict):\n",
        "    instrument_index = 0\n",
        "    files_read = 0\n",
        "    for instrument in instruments:\n",
        "        for filename in os.listdir(directoryName+\"/\"+instrument):\n",
        "            if filename.endswith('.wav'): # only get MFCCs from .wavs\n",
        "                # read in our file\n",
        "                (sig,rate) = ld_wav(directoryName +\"/\"+instrument+\"/\" +filename)\n",
        "                sig = zeroPadding(sig)\n",
        "                # get mfcc\n",
        "                spectogram = np.abs(librosa.stft(sig,n_fft=512))\n",
        "                mfcc_feat = ft_extraction.mfcc(S=spectogram,sr=rate,n_mfcc=13)\n",
        "                rolloff_feat = ft_extraction.spectral_rolloff(S=spectogram,sr=rate)\n",
        "                bandwidth_feat = ft_extraction.spectral_bandwidth(S=spectogram,sr=rate,)\n",
        "                centroid_feat = ft_extraction.spectral_centroid(S=spectogram,sr=rate)\n",
        "                zero_crossing_rate_feat = ft_extraction.zero_crossing_rate(y=sig,frame_length=512,hop_length=512//4)\n",
        "                rms_feat = ft_extraction.rms(S=spectogram)\n",
        "                slope_feat = FeatureSpectralSlope(spectogram,rate)\n",
        "                slope_feat = np.array(slope_feat).reshape(1,-1)\n",
        "                kurtosis_feat = FeatureSpectralKurtosis(spectogram,rate)\n",
        "                kurtosis_feat = np.array(kurtosis_feat).reshape(1,-1)\n",
        "                skewness_feat = FeatureSpectralSkewness(spectogram,rate)\n",
        "                skewness_feat = np.array(skewness_feat).reshape(1,-1)\n",
        "                data_dict[\"instrument\"].append([instrument_index])\n",
        "                for i in range(0,13):\n",
        "                  data_dict[\"mfcc\"+str(i)].append(mfcc_feat[i])\n",
        "                data_dict[\"rolloff\"].append(rolloff_feat[0])\n",
        "                data_dict[\"bandwidth\"].append(bandwidth_feat[0])\n",
        "                data_dict[\"centroids\"].append(centroid_feat[0])\n",
        "                data_dict[\"zero_crossing_rate\"].append(zero_crossing_rate_feat[0])\n",
        "                data_dict[\"rms\"].append(rms_feat[0])\n",
        "                data_dict[\"slope\"].append(slope_feat[0])\n",
        "                data_dict[\"kurtosis\"].append(kurtosis_feat[0])\n",
        "                data_dict[\"skewness\"].append(skewness_feat[0])\n",
        "                # create a file to save our results in\n",
        "                files_read += 1\n",
        "        instrument_index += 1\n",
        "    data_dict = includeDeltaFeat(data_dict)\n",
        "    return data_dict                                                                                           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQlwBN4FMsv7"
      },
      "source": [
        "\"\"\"data_dict = getData(directoryName,instruments,data_dict)\n",
        "with open(directoryName+\"/data_dict_procVoz.pickle\",\"wb\") as f:\n",
        "  pickle.dump(data_dict,f)\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06iZIxnjN1Bv"
      },
      "source": [
        "with open(directoryName+\"/data_dict_procVoz.pickle\",\"rb\") as f:\n",
        "  data_dict = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFtJku-NswWr"
      },
      "source": [
        "len(data_dict[\"bandwidth\"][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oJS6GMmMswC"
      },
      "source": [
        "np.count_nonzero(~np.isnan(data_dict[\"mfcc0\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWOC2oI7MswU"
      },
      "source": [
        "for key in data_dict.keys():\n",
        "    print(key,len(data_dict[key]))\n",
        "    #convert list to np\n",
        "    data_dict[key] = np.array(data_dict[key])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9tSjFXyWRPb"
      },
      "source": [
        "data_dict[\"rolloff\"].shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JypyHHZMswd"
      },
      "source": [
        "#Para o datafram não aceita matriz como input. Como cada arquivo de audio do data se obtem uma matriz de \n",
        "#tamanho (tam,n_features) multiplicaremos cada elemento do vetor de saída por esse tamanho. Assim, para um dado audio e para cada linha\n",
        "#da matriz de dados teremos um valor de saída corresponde. A representação com matriz será util quando utilizarmos redes neurais\n",
        "#O metodo flatten() transforma 2d arrays em 1d\n",
        "data_for_df = dict()\n",
        "data_for_df['instrument'] = list()\n",
        "for key in data_dict.keys():\n",
        "  if key != 'instrument':\n",
        "    data_for_df[key] = data_dict[key].flatten()\n",
        "    print(\"Feat {} has len {}\".format(key,len(data_for_df[key])))\n",
        "  else:\n",
        "    for i in range(len(data_dict['instrument'])):\n",
        "      data_for_df['instrument'].extend([data_dict['instrument'][i][0]] * len(data_dict['bandwidth'][i]))\n",
        "df = pd.DataFrame.from_dict(data_for_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XotAd8utyGf7"
      },
      "source": [
        "len(data_dict[key].flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLXHshLUyN6e"
      },
      "source": [
        "517*1301"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42MaTZw0leJd"
      },
      "source": [
        "(1301x130)x31"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ris7o6fMMswk"
      },
      "source": [
        "input_variables = list(df.columns)\n",
        "input_variables.remove(\"instrument\")\n",
        "output_variable = \"instrument\"\n",
        "print(input_variables)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faZQrcLr4ekW"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maqIlNcjMsyE"
      },
      "source": [
        "corrmat = df.corr()\n",
        "f, ax = plt.subplots(figsize=(12, 9))\n",
        "sns.heatmap(corrmat,cmap=sns.color_palette(\"RdBu_r\", 1000), vmin=-1,vmax=1, square=True)\n",
        "plt.savefig('CorrelationMatrix.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SkRhLDU9-o"
      },
      "source": [
        "np.unique(df['instrument'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pg17kwOQMswx"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U37TWRqsMsws"
      },
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(instruments)\n",
        "print(le.inverse_transform([0,1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66PQorIC9d4i"
      },
      "source": [
        "from pyspark.ml.feature import PCA,MinMaxScaler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEHgGNLOfDJh"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df[input_variables], df[output_variable], test_size=0.33, random_state=42)\n",
        "#Transform to Spark DataFrame\n",
        "df_train = map(lambda x: (int(x[0]), Vectors.dense(x[1:])), np.column_stack((y_train,X_train)))\n",
        "df_train = spark.createDataFrame(df_train,schema=[\"label\", \"features\"])\n",
        "df_test = map(lambda x: (int(x[0]), Vectors.dense(x[1:])), np.column_stack((y_test,X_test)))\n",
        "df_test = spark.createDataFrame(df_test,schema=[\"label\", \"features\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h0HgQTV9_Me"
      },
      "source": [
        "#Apply Min-Max Scaler\n",
        "mmScaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled\")\n",
        "#Apply PCA\n",
        "pca = PCA(k=10, inputCol=mmScaler.getOutputCol(), outputCol=\"pca_features\")\n",
        "#Set Classifier\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=pca.getOutputCol(), numTrees=10)\n",
        "pipeline = Pipeline(stages=[mmScaler, pca, rf])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COFOkjTMDz34"
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPE2Nl7YmBPp"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "111bxsxvq6HO"
      },
      "source": [
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",metricName=\"areaUnderPR\")\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(rf.numTrees, [8, 16, 32]) \\\n",
        "    .addGrid(rf.maxDepth, [2, 4, 8]) \\\n",
        "    .build()\n",
        "\n",
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=5) \n",
        "\n",
        "# Run cross-validation, and choose the best set of parameters.\n",
        "cvModel = crossval.fit(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WKpgG3x_XCS"
      },
      "source": [
        "prediction = cvModel.transform(df_test)\n",
        "evaluator.evaluate(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dc55MHZmrL5"
      },
      "source": [
        "# Neural Networks "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_K2F7NQhfpd"
      },
      "source": [
        "#Convert DICT to np.array by concatenating columns\n",
        "X = np.array([data_dict[key] for key in input_variables])\n",
        "Y = data_dict['instrument'].flatten()\n",
        "# We have a sequence of feature vectors of size n_features \n",
        "# Agora, para uma determina sequencia de caracteristicas extraidas efetuaremos a classificação. Logo teremos agora uma matrix:\n",
        "# (tam_seq,n_features). Consideraremos cada linha dessa matriz sendo o tempo\n",
        "#X.shape == (31,1301,130)\n",
        "#Mas queremos que X.shape = (1301,130,31)\n",
        "X = np.array([A.flatten() for A in np.transpose(X,(1,2,0))])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y,\n",
        "                                                    test_size=0.33,\n",
        "                                                    random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVxMbvCC_ByD"
      },
      "source": [
        "df_train = map(lambda x: (int(x[0]), Vectors.dense(x[1:])), np.column_stack((y_train,X_train)))\n",
        "df_train = spark.createDataFrame(df_train,schema=[\"label\", \"features\"])\n",
        "df_test = map(lambda x: (int(x[0]), Vectors.dense(x[1:])), np.column_stack((y_test,X_test)))\n",
        "df_test = spark.createDataFrame(df_test,schema=[\"label\", \"features\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fglqEqHr9nE"
      },
      "source": [
        "from elephas.spark_model import SparkModel\n",
        "from elephas.utils.rdd_utils import to_simple_rdd\n",
        "from elephas.ml_model import ElephasEstimator\n",
        "from tensorflow.keras import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYmk9RUsrAow"
      },
      "source": [
        "#rdd = to_simple_rdd(sc, X_train, y_train)\n",
        "opt = optimizers.Adam(lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkPj7BNjLdto"
      },
      "source": [
        "def get_CNN_Model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(layers.Reshape((517,33,1), input_shape=(input_shape,) ) )\n",
        "    assert model.output_shape == (None, 517, 33, 1)\n",
        "    model.add(layers.Conv2D(32,(3,3),activation='relu',padding=\"same\",strides=(1,1)))\n",
        "    model.add(layers.MaxPooling2D((2,2),padding='same'))\n",
        "    model.add(layers.Conv2D(64,(3,3),activation='relu',padding=\"same\",strides=(1,1)))\n",
        "    model.add(layers.MaxPooling2D((2,2),padding='same'))\n",
        "    model.add(layers.Conv2D(64,(3,3),activation='relu',padding=\"same\",strides=(1,1)))\n",
        "    model.add(layers.MaxPooling2D((2,2),padding='same'))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(64,activation='relu'))\n",
        "    model.add(layers.Dense(2,activation='softmax'))\n",
        "    #model.summary()\n",
        "    #model.compile(optimizer=opt,loss='binary_crossentropy')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwfh_32qBWn_"
      },
      "source": [
        "X_train.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4yNgy6YvJDD"
      },
      "source": [
        "inp = X_train.shape[1]\n",
        "#inp.append(1)\n",
        "model = get_CNN_Model(inp)\n",
        "model.compile(optimizer=opt,loss='categorical_crossentropy')\n",
        "opt_conf = optimizers.serialize(opt)\n",
        "# Initialize SparkML Estimator and set all relevant properties\n",
        "estimator = ElephasEstimator()\n",
        "estimator.setFeaturesCol(\"features\")             # These two come directly from pyspark,\n",
        "estimator.setLabelCol(\"label\")                 # hence the camel case. Sorry :)\n",
        "estimator.set_keras_model_config(model.to_yaml())       # Provide serialized Keras model\n",
        "estimator.set_categorical_labels(True)\n",
        "estimator.set_nb_classes(2)\n",
        "estimator.set_num_workers(1)  # We just use one worker here. Feel free to adapt it.\n",
        "estimator.set_epochs(20) \n",
        "estimator.set_batch_size(32)\n",
        "estimator.set_verbosity(1)\n",
        "estimator.set_validation_split(0.15)\n",
        "estimator.set_optimizer_config(opt_conf)\n",
        "estimator.set_mode(\"synchronous\")\n",
        "estimator.set_loss(\"categorical_crossentropy\")\n",
        "estimator.set_metrics(['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1SiORZoDBun"
      },
      "source": [
        "fitted = estimator.fit(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEVGfOgiJR6p"
      },
      "source": [
        "prediction = fitted.transform(df_test) # Evaluate on train data.\n",
        "pnl = prediction.select(\"label\", \"prediction\")\n",
        "pnl.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzJcwmCSJ5jO"
      },
      "source": [
        "prediction.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIw-4u_2fZVi"
      },
      "source": [
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
        "evaluator.evaluate(prediction,{evaluator.metricName: \"areaUnderPR\"})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}